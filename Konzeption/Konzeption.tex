\section{\acf*{MFEM}}\label{MFEM}

Die Bewertung, ob ein Framework für den produktiven Einsatz in einer Microservice Architektur geeignet ist, lässt sich nicht auf die Servicearchitektur beschränken. Aus diesem Grund ist die \ac{MFEM} keine reine Architekturbewertungsmethode.\\ 
Sie betrachtet drei Seiten der Frameworkbewertung, welche in \ref{BewertungFrameworkSeiten} dargestellt sind.

\image[FrameworkSeiten.pdf][width=0.8\linewidth]{BewertungFrameworkSeiten}{Bewertung der drei Frameworkseiten}{Die 3 Seiten eines Frameworks und dessen reguläre Bewertung}

Wie ein Framework benutzt wird, stellt die erste Seite \enquote{Nutzung} dar. Dies umfasst z.~B. die Art und Weise wie Funktionen mit dem Framework umgesetzt werden, wie stark es den Nutzer dabei unterstützt oder welche Programmiersprache genutzt werden muss. Zusätzlich beinhaltet es den Architektureinfluss. Da ein Framework einen Rahmen zur Verfügung stellt, haben die Architekturentscheidungen dessen einen Einfluss auf die Architektur des Microservices.\\
Des Weiteren kann die Umsetzung des Frameworks selber bewertet werden. Somit betrachtet die zweite Seite \enquote{Produktqualität} die Qualität des Frameworks an sich, welche im Regelfall von der Qualitätssicherung des Herstellers abgesichert werden sollte. Doch hängt dies stark an den verwendeten Anforderungen. So können diese von den Anforderungen des Nutzers abweichen und somit nicht die benötigte Qualität liefern.\\
Die dritte Seite, \enquote{Zukunftssicherheit}, betrachtet das Framework über den gesamten Software-Lebenszyklus. Die Entscheidung für ein Framework sollte auch auf lange Sicht bestand haben. So sollten z.~B. Fehler vom Hersteller behoben oder neue Funktionen eingebaut werden.\\

\ac{MFEM} setzt dabei keine grundlegenden Kenntnisse über das Framework voraus, da es zwei Seiten der Analyse anbietet. Dies ist einerseits die Nutzung von bereits vorhandenem Expertenwissen, was die Phase der Evaluation verkürzt. Andererseits werden die restlichen Daten über die Erstellung von Prototypen erfasst.
Auf dieser Basis kann am Ende eine begründete Entscheidung für oder gegen den Einsatz eines Frameworks getroffen werden.

Der Ablauf von \ac{MFEM} ist in Bild \ref{MFEMAblauf} schematisch dargestellt und startet mit der optionalen Kickoff-Phase. Diese kann somit auch übersprungen werden, wenn die beteiligten Personen nur die Software-Architekten selbst sind. Sollten bei der Bewertung auch weitere Stakeholder mit einbezogen werden, wird zur Durchführung der Kickoff-Phase geraten.

\image[MFEM-Phasen.pdf][width=0.75\linewidth]{MFEMAblauf}{Ablaufschema MFEM}{Phasen der Frameworkbewertung nach \ac{MFEM}}

\subsection{Kickoff-Phase}

In der Kickoff Phase geht es darum, Klarheit zu schaffen. Den Beteiligten Personen sollten der Ablauf und die Ziele dieser Methode erläutert werden. Wie auch schon bei \ac{ATAM} ist das Ziel von \ac{MFEM} nicht die Vergabe von Noten. Vielmehr geht es darum den Reifegrad eines Frameworks und etwaige Risiken, wie erhöhten Entwicklungsaufwand für fehlende Funktionen, festzustellen. 

In diesem Zusammenhang sollte auch die vorhandene Microservice Architektur vom Soft\-ware\-architekten vorgestellt werden, damit die Grundlage der Bewertung klar ist. Neben den in der Architektur getroffenen Entscheidungen geht es auch um technische Einschränkungen und Abhängigkeiten zu Drittsystemen. Auf dieser Basis kann die Architektur analysiert und Anforderungen definiert werden.

\subsection{Analysephase}

\image[MFEM-Analysephase.pdf][width=0.4\linewidth]{MFEM-Analysephase}{MFEM-Analysephase}{Analysephase von \ac{MFEM}}

\subsubsection{Architektur Analyse und Wahl der Anforderungen}\label{Analyse_Wahl_Anforderungen}

Aus dem vorangegangenen Kapitel wird ersichtlich, dass eine Bewertung nicht nur frühzeitig erfolgen muss. Viel mehr ist es wichtig, dass diese auch auf detaillierten Anforderungen basiert. Durch die maßgeblichen Stakeholder werden diese vorgegeben. In einer Microservice Architektur sind dies die definierten Umstände eines Services. Diese beschränken sich dabei nicht auf die Kommunikation zwischen zwei einzelnen Services. Gerade die Komposition der Microservices über die Infrastruktur muss gewährleistet sein. Wobei Punkte wie Sicherheit und Wartbarkeit die Komplexität noch steigern.
Damit ein Service dies erfüllt und somit in der Architektur funktionieren kann, müssen anhand der Umstände, wie z.~B. Servicediscovery, Logging oder Tracing, Anforderungen definiert werden. Nur wenn das Framework eines Services diese unterstützt, kann garantiert werden, dass es in die Gesamtarchitektur passt.

\newparagraph{Funktionale Frameworkanforderungen}
\
Funktionale Frameworkanforderungen sind demnach die Anforderungen, die von der Architektur vorgegeben werden. Sie sind demnach stark von der Umsetzung dieser abhängig. Ein gutes Beispiel hierfür ist die Servicediscovery.\\ 
Damit sich Services gegenseitig finden können, ohne die Flexibilität der Architektur zu verlieren, darf eben diese nicht fehlen. Die Umsetzung kann dabei jedoch stark variieren. So kann ein zentraler Service, wie z.~B. Netflix-Eureka\footnote{
\url{https://github.com/Netflix/eureka}
}
oder Consul\footnote{
\url{https://www.consul.io}
}, 
eine Registrierungsstelle anbieten. Dort können sich sämtliche Service-Instanzen registrieren und auch andere Services finden. Diese müssen eine Servicediscovery aber auch nicht direkt unterstützen. Projekte wie Spring Cloud Sidecar\footnote{
\url{http://projects.spring.io/spring-cloud/spring-cloud.html\#_polyglot_support_with_sidecar}
}
übernehmen dies für einzelne Services. Die Discovery kann aber auch auf die darüber liegende Abstraktions-Schicht hochgezogen werden. Innerhalb eines Kubernetes\footnote{
\url{http://kubernetes.io}
}
Clusters übernimmt die Service-Verwaltung diese Aufgabe selbst und stellt sie über Umgebungsvariablen oder DNS zur Verfügung. 

\image[Servicediscoverytypen.pdf]{Servicediscoverytypen}{Servicediscovery Typen}{Beispiele für Ausprägungen der Servicediscovery}

Die spezifische Ausprägung der Architektur gibt somit Anforderungen vor, die es zu identifizieren gilt. Diese stellen die funktionalen Frameworkanforderungen dar und müssen für die Akzeptanz erfüllt werden. Sollte sich mit einem Framework z.~B. nicht die benötigte Authentifizierung umsetzen lassen, kann der Service nicht vor Missbrauch geschützt werden. Ein Einsatz im produktiven Umfeld wäre demnach undenkbar oder nur mit unverhältnismäßig großem Aufwand zu realisieren.

\newparagraph{Nicht-Funktionale Frameworkanforderungen}
\
Neben den funktionalen Frameworkanforderungen lassen sich auch nicht-funktionale Anforderungen definieren, die zur Erreichung und Zufriedenstellung der benötigen Funktionen notwendig sind. Dies muss dabei nicht auf die durch das Framework vorgegebene Architektur des Services beschränkt sein. An dieser Stelle kann das Framework als Werkzeug aufgefasst werden. Es gilt somit nicht nur die Frage zu klären, ob ein Framework eine benötigte Funktion bereitstellt. Sondern ob es den Entwickler bei der Umsetzung bestmöglich unterstützt und dabei flexibel bleibt.\\ 
Nach dem KISS-Prinzip\footnote{
\url{http://principles-wiki.net/principles:keep_it_simple_stupid}
} 
wird zum Beispiel die Funktion bevorzugt, die möglichst einfach und \enquote{dumm} erscheint. So brauch es für die Umsetzung keine anspruchsvollen oder besonders cleveren Lösungen. Eine Einfache lässt sich nicht nur besser Lesen und Verstehen, es erhöht auch die Wartbarkeit.\\
Bei einem Framework kann dies z.~B. bedeuten, dass die Konfiguration einer Funktion mit nur wenigen Funktionsaufrufen erfolgt und dabei stets nachvollziehbar bleibt. Im Idealfall kann man dies direkt am Code erkennen, ohne dabei Kenntnisse über das Framework zu haben.\\
Des Weiteren fordern die nicht-funktionalen Frameworkanforderungen wie strukturiert die Umsetzung damit ist. Man sollte z.~B. beim Ändern eines Endpunktes nur einen Codeabschnitt anfassen müssen und nicht viele verschiedene. 

\newparagraph{Basisanforderungen von \ac{MFEM}}
\
Damit das Bewertungsteam bei der Findung von funktionalen und nicht-funktionalen Frameworkanforderungen nicht bei null anfangen muss, wird an dieser Stelle eine Orientierungshilfe angeboten. Hier wurde in einem Brainstorming versucht, eine Schnittmenge an Anforderungen zu finden, die für die meisten Microservice Architekturen gelten sollte. 
Die so gefundenen Anforderungen basieren auf der Erfahrung der Anwendungsentwicklung innerhalb der Landeshauptstadt München. Diese hat sich in den letzten Jahren mit dem produktiven Einsatz von Microservices beschäftigt und diverse erfolgreiche Projekte damit umgesetzt.
\ac{MFEM} enthält somit Basisanforderungen, die als Grundlage für die Bewertung eines Frameworks hergenommen werden können. Eine Verpflichtung zu diesen Anforderungen besteht jedoch nicht. Wie zuvor beschrieben, hängen die Anforderungen stark am jeweiligen Kontext und müssen dahingehend beurteilt werden. Daher ist essentiell, dass die Basisanforderungen nur als Orientierung dienen und beliebig gekürzt aber in jedem Fall um weitere kontext-spezifische Anforderungen erweitert werden müssen.  

Um die Analyse und Wahl der Anforderungen zu unterstützen, wird hier ein Quality Utility Tree aufgebaut.

\newparagraph{Entwicklung Quality Utility Tree}
\
Der \ac{QUT} bietet einen effizienten Top-Down Ansatz zur Identifizierung und Verfeinerung von Qualitätsanforderungen. Mittels Brainstorming sollen Anforderungen gefundenen und in einzelne Kategorien aufgeteilt werden. Die Anforderungen müssen dabei nicht sofort konkret sein, sondern können anfangs grob erstellt werden. Über den Baum können die groben Anforderungen nach rechts weiter verfeinert werden, bis diese konkret genug sind. 

Bei der Kategorisierung kann das Qualitätsmodell aus der ISO/IEC 25010 hergenommen werden. Wobei das Modell nur eine Hilfestellung ist und keine Vorgabe darstellt. Es geht lediglich darum einen Überblick zu schaffen und mehrere Teilaspekte zu betrachten. Auch ermöglichen die Kategorien eine bessere Übersicht über das Endergebnis. Hierzu später mehr im Abschnitt \ref{Ergebnisse_auswerten} \enquote{Ergebnisse auswerten}.

Für die Basisanforderungen wurden die Kategorien Funktionalität, Performance, Benutzbarkeit, Sicherheit, Wartbarkeit und Zukunftssicherheit definiert. Anschließend wurden diesen die Qualitätsanforderungen zugeordnet und stellen ein Beispiel dar, wie ein entsprechender \ac{QUT} aufgebaut ist. Bild \ref{MSQUTAusschnitt} zeigt einen Ausschnitt des gesamten Baumes, wobei die vollständige Abbildung im Anhang zu finden ist. 

\image[MSQUTAusschnitt.pdf][width=0.7\linewidth]{MSQUTAusschnitt}{\acs{MFEM} \acl*{QUT} Beispiel}{Ausschnitt aus dem \acl*{QUT} der Basisanforderungen}

Damit der Fokus in späteren Phasen auf den wichtigen Anforderungen liegt, werden anschließend alle mit Prioritäten versehen. Hierbei bietet sich eine dreistufige Skala mit A, B und C an. Dies sollte für den Normalfall ausreichen und benötigt keiner weiteren Detailstufe.
Die mit A bewerteten Anforderungen können zudem später zu einem schnelleren Abbruch führen, wenn diese nicht erfüllt werden. Auch wirken sie sich beim Gesamtergebnis stärker aus als niedriger bewertete Anforderungen. 

Der in Bild \ref{MSQUTAusschnitt} gezeigte \ac{QUT} wurde im Bild \ref{MSQUTPrio} um Prioritäten erweitert. Wobei dies nur ein Beispiel ist und bei jeder Bewertung separat durchgeführt werden sollte.

\image[MSQUTPrioAusschnitt.pdf][width=0.7\linewidth]{MSQUTPrio}{\acs{MFEM} \acl*{QUT} Beispiel mit Prioritäten}{Beispiel eines \acl*{QUT} der Basisanforderungen mit Prioritäten}

\subsubsection{Metriken definieren über \ac*{GQM}}

In diesem Schritt müssen für die identifizierten Qualitätsanforderungen möglichst aussagekräftig Metriken gefunden werden. Dies können dabei Messung an der Architektur sein, wie das Vorhandensein spezifischer Funktionen oder Komponenten sowie den Einsatz bewährter Entwurfsmuster. Dabei können die möglichen Ergebnisse komplexer als nur ein einfaches \enquote{enthalten} oder \enquote{nicht enthalten} sein. Falls beispielsweise eine spezifische Funktion vom Framework nicht unterstützt wird, heißt dies nicht, dass sich diese nicht umsetzen lässt. Hier bietet sich eine Ordinalskala an. Wobei folgende mögliche Ergebnisse verwendet werden können: enthalten, leicht umsetzbar, schwer umsetzbar, nicht möglich. Dieser Ansatz bietet den Vorteil, dass das Ergebnis quantifizierbar ist.

Diese Metriken bieten sich auch für weitere Anforderungen an, die sich schwer in Zahlen erheben lassen. Die Tabelle \ref{OrdinalskalaBeispiele} zeigt ein paar Beispiele aus den Basisanforderungen und die zugehörigen Ordinalskalen mit einer Erläuterung zu den jeweiligen Stufen.

\begin{table}[!htb]
	\centering
	\resizebox{\linewidth}{!}{
	\begin{tabular}{p{3cm}|p{3cm}|p{2cm}|p{8cm}}

		\textbf{Anforderung (Goal)}                   & \textbf{Frage (Question)} & \textbf{Skala für Metrik} & \textbf{Erläuterung}                                                                                         \\ \hline
		\multirow{3}{3cm}{Effiziente Entwicklung}	& 
		\multirow{3}{3cm}{Kann das Framework schnell erlernt werden?} 
		& sehr gut & Das Framework lässt sich schnell erlernen und ist intuitiv zu bedienen. Es nimmt dem Nutzer, wo es geht, Arbeit ab. 
		\\ \cline{3-4} & 
		& gut & Die Einarbeitungszeit ist etwas länger und das Framework ist nicht sehr intuitive.                                   
		\\ \cline{3-4} &                                                                                       
		& schlecht & Sehr lange Einarbeitungszeit und viele Eigenheiten, die sich dem Benutzer nicht direkt erschließen.                  
		\\ \hline
		\multirow{3}{3cm}{Standard Übertragungs-formate} & 
		\multirow{3}{3cm}{Wird JSON unterstützt?}                                               
		& automatisch & Das Framework deserialisiert und serialisiert Objekte automatisch ins JSON Format und zurück.                                              
		\\ \cline{3-4} & 
		& manuell & Durch eigenes Parsing oder mittels Drittbibliotheken können die Objekte de- und serialisiert werden.                 
		\\ \cline{3-4} &                                                                                       
		& nicht möglich & JSON wird nicht unterstützt und kann auch nicht nachträglich integriert werden.                                      
		\\ \hline
		\multirow{4}{3cm}{Gute Dokumentation} & 
		\multirow{4}{3cm}{Bietet das Framework eine umfangreiche Dokumentation mit Beispielen?} 
		& sehr umfangreich mit Beispielen & Die Dokumentation erklärt jeden Teilaspekt des Frameworks und bietet viele Code-Beispiele.                           
		\\ \cline{3-4} &                                                                                       
		& umfangreich & Sämtliche Funktionen sind von der Dokumentation erfasst.                                                            
		\\ \cline{3-4} &                                                                                       
		& einfach & Nur die wichtigsten Funktionen werden abgedeckt.                                                                     
		\\ \cline{3-4} &                                                                                       
		& nicht vorhanden & Es ist keine Dokumentation vorhanden.                                                                                
		\\ \hline
	\end{tabular}
	}
	\caption[Beispiele für Ordinalskalen]{Beispiel: Anforderungen und Ordinalskalen}
	\label{OrdinalskalaBeispiele}
\end{table}
\FloatBarrier

Zusätzlich können auch Softwaremetriken genutzt werden. Da im Zuge der Evaluation ein Prototyp erstellt wird, können an diesem auch direkt Messungen vorgenommen werden. Damit kann neben der Performance auch z.~B. der Aufwand zur Umsetzung bestimmter Funktionen bemessen werden. Falls z.~B. die Erstellung eines \ac{REST}-Endpunktes sehr aufwändig ist und viele Codezeilen sowie Methodenaufrufe benötigt, macht dies den Microservice bei vielen Endpunkten schnell sehr komplex. Zudem verschlechtert es die Wartbarkeit.
Um bei der Definition der Metriken stets das Ziel im Auge zu behalten, wird die bereits bekannte \ac{GQM} Methode genutzt. 

Eine um Metriken erweiterter Qualitätsbaum ist in Bild \ref{MFEMGQMQUTAusschnitt} dargestellt. Wie zuvor ist dies nur ein Ausschnitt, wobei der gesamte Baum im Anhang zu finden ist.

\image[MFEMGQMQUTAusschnitt.pdf]{MFEMGQMQUTAusschnitt}{\acs{MFEM} Qualitätsbaum erweitert um Metriken}{Ein Ausschnitt aus dem Qualitätsbaum erweitert um Metriken}

\subsection{Evaluationsphase}

\image[MFEM-Evaluationsphase.pdf][width=0.4\linewidth]{MFEM-Evaluationsphase}{MFEM-Evaluationsphase}{Evaluationsphase von \ac{MFEM}}

Während der Evaluation wird das Framework auf die Anforderungen mittels der zuvor definierten Metriken untersucht. Dabei wird ein tieferer Blick in das Framework geworfen. Neben der Untersuchung von Dokumentation und bereits vorhandenem Expertenwissen, wird auch ein Prototyp erstellt. Dieser soll als Referenz für zukünftig entwickelte Services gelten und muss somit zielgerichtet entwickelt werden. Es gilt in kürzester Zeit essenzielle Funktionen umzusetzen und dabei Erkenntnisse über das Framework zu gewinnen. Um dies zu Unterstützen muss im ersten Schritt die Evaluation genauer definiert werden.

\subsubsection{Evaluation definieren}\label{Evaluation_definieren}

In der Softwareevaluation wird zwischen der subjektiven und objektiven Evaluation unterschieden\cite{Hegner2016}.
Dabei versucht die subjektive Evaluation eine Beurteilung durch den Benutzer zu finden, wobei der Benutzer im Fall von \ac{MFEM} der Softwareentwickler ist. Während dieser Evaluation werden sogenannte \enquote{weiche} Daten erhoben. Diese sagen aus, ob die Entwicklung mit dem Framework effizient, einfach, klar oder einsichtig ist. Es wird somit versucht eine Aussage über die Akzeptanz zu treffen.
Die subjektiven Eindrücke versucht man bei der objektiven Evaluation möglichst auszuschalten. Das gelingt durch das Anwenden von \enquote{harten} Methoden zur Erhebung von quantitativen, statisch abgesicherten Daten\cite{Hegner2016}. Sehr gute Beispiele hierfür sind Performancemessungen, Fehlerraten oder Komplexitätsbestimmungen.  

\newparagraph{Subjektive Evaluation}
\
Während der subjektiven Evaluation soll bei \ac{MFEM} der Prototyp von einem Softwareentwickler erstellt werden. Wie eingangs erwähnt muss dies zielgerichtet ablaufen. Um dies zu unterstützen und gleichzeitig eine Aussage über Qualitätsmerkmale wie Lernbarkeit, Analysierbarkeit, Wartbarkeit oder Benutzbarkeit zu treffen, soll der \ac{CW} verwendet werden. Dies ist eine aufgabenorientierte Inspektionsmethode\cite{Hegner2016}. Dabei versetzt sich der Prüfer in einen hypothetischen Benutzer und analysiert konkrete Handlungsabläufe. Dies kann z.~B. das Lösen eines Problems oder das Umsetzen einer einfachen Funktion sein. 
So kann er feststellen, woran die Entwicklung eines Microservices mit Hilfe des Frameworks vermutlich scheitern wird.

Der \ac{CW} verläuft in 3 Schritten\cite{Hegner2016}:

\begin{description}[leftmargin=!,labelwidth=\widthof{\bfseries Input definieren}]
	\item[1. Vorbereitung]
	In diesem Schritt werden die Beispielszenarien definiert. D.~h. es werden für den Prüfer Aufgaben bzw. Problemstellungen festgelegt, die durchgeführt oder gelöst werden sollen. Dies können grundlegend Aufgaben sein, wie das Erstellen eines \ac{REST}-Endpunktes. Sie sollten so präzise wie möglich formuliert werden.
	\item[2. Analyse]
	Während der Analyse nimmt sich der Prüfer ein Szenario nach dem anderen zur Hand und führt dieses durch. Für jede Aktivität wird ein Protokoll angefertigt. Durchgeführten Aktionen und damit eventuell zusammenhängenden Probleme oder Feststellungen sind damit festzuhalten.
	\item[3. Follow Up]
	Die durch die Analyse gewonnenen Erkenntnisse werden zusammengefasst und ausgewertet. Falls nötig, können Maßnahmen bestimmt werden.
\end{description}

Der Vorteil des \ac{CW} ist die schnelle und einfache Anwendung. Zudem kann es, wie von \ac{MFEM} benötigt, in einem frühen Stadium der Entwicklung verwendet werden.\cite{Wharton1994} 
Im derzeitigen Definitionsschritt müssen in erster Linie Szenarien definiert werden. Die Analyse und das Follow Up erfolgen in den nächsten Schritten.

\newsubparagraph{Definition von Szenarien}\label{Definition_Szenarien}

Für die Definition der Szenarien muss ein Standard-Anwendungsfall gefunden werden. Hierzu sollten sich die Prüfer überlegen, wie ein herkömmlicher Service, der mit dem Framework erstellt werden soll, aussehen könnte und was er für Eigenschaften haben sollte. Da der Einsatzzweck stark an die zuvor definierten Anforderungen gebunden ist, können diese für die Definition genutzt werden. Soll der Service z.~B. eine eigene Datenbank verwalten und mittels \ac{REST} nach außen anbieten oder wird Event Sourcing eingesetzt? Vielleicht soll der Service aber auch keine eigenen Daten verwalten, sondern von anderen Services einholen und zusammenführen.

Die Definition des Standard-Anwendungsfalls kann erst einmal grob erfolgen und wird im Folgenden weiter verfeinert. Hierzu sollte der gefundene Anwendungsfall in Entwicklungsstufen aufgeteilt werden. Diese stellen anschließend die Szenarien dar. Die Anzahl sollte dabei gering gehalten werden (z.~B. 3 Szenarien), damit der Evaluationsaufwand nicht zu groß wird. Um die Szenarien weiter zu verfeinern und einen Rückschluss auf die Anforderungen zu gewährleisten, werden anschließend den Szenarien die zu messenden Metriken zugeordnet. Dabei kann es vorkommen, dass sich einzelne Metriken nicht zuordnen lassen, da die Szenarien anfangs nur grob definiert sind. In diesem Fall müssen die Szenarien in Hinblick auf die fehlenden Metriken weiter verfeinert werden. Der dadurch entstehende Kreislauf ist im Bild \ref{MFEM-Evaluationskreislauf} abgebildet.       

\image[MFEM-Evaluationskreislauf.pdf][width=0.5\linewidth]{MFEM-Evaluationskreislauf}{MFEM Kreislauf Evaluation}{Kreislauf zur Definition und Verfeinerung der Szenarien}

Bei der Zuordnung der Metriken muss zwischen objektiver und subjektiver Evaluation unterschieden werden. Aus diesem Grund wird der Vorgang erst im Abschnitt \ref{Metriken_zuordnen} genauer erläutert, nachdem auch die objektive Evaluation definiert wurde.

\newsubparagraph{Beispiel-Szenarien}

Im Folgenden werden 3 Beispiel-Szenarien definiert, die den Anwendungsfall eines einfachen Daten-Service darstellen sollen. Dieser hat eine Anbindung an eine Datenbank und stellt das enthaltene Datenmodell mittels \ac{REST} zur Verfügung. Mit dieser groben Definition werden die 3 Szenarien gebildet.

\textbf{Szenario 1 - Installation:} Die Grundlage für den Einsatz eines Frameworks ist die Installation der benötigten Komponenten. Da \ac{MFEM} sprachunabhängig ist, kann nicht davon ausgegangen werden, dass alle beteiligten Entwickler bereits die benötigen Compiler bzw. Interpreter und zugehörige Bibliotheken installiert haben. Zudem werden in Zeiten von Continuous Integration\footnote{\url{https://de.wikipedia.org/wiki/Kontinuierliche_Integration}} und immer kürzer werdenden Time-to-Market Zyklen automatische Build-Tools benötigt. Die Einrichtung dieser Komponenten ist somit die Basis einer jeden Entwicklung und stellt das erste Szenario dar.

\image[Szenario1.pdf][width=0.5\linewidth]{Installation}{Installation eines Frameworks}{Szenario 1: Installation des Frameworks mit allen benötigten Komponenten.}

\textbf{Szenario 2 - einfacher Service:} Die ersten Schritte, gerade in einem ungewohnten Umfeld, sollten nicht zu groß gewählt werden, damit ein erster Eindruck zur Struktur und Syntax erfolgen kann. So bildet das zweite Szenario die Erstellung eines einfachen Hello-World-Services. Dieser soll einen Endpunkt enthalten, der auf jegliche Anfrage \enquote{Hello World!} antwortet. Als Ergebnis werden so erste Erkenntnisse über die Funktionsweise des Frameworks und der Erstellung von grundlegenden Elementen eines Microservices gewonnen. Da die Absicherung des Services ein wesentlicher Bestandteil ist, soll anschließend der Endpunkt mit einer Authentifizierung abgesichert werden. Die genaue Umsetzung hängt dabei von den zuvor definierten Anforderungen ab. 
Mit den Basis-Anforderungen von \ac{MFEM} wird eine Authentifizierung mittels OAuth2-Token gefordert. Zur Vereinfachung werden \ac{JWT}\footnote{\url{https://jwt.io}} eingesetzt, da diese signiert sind und vom Service, ohne weitere Kommunikation mit anderen Services, selbst validiert werden können.   

\image[Szenario2.pdf][width=0.6\linewidth]{EinfacherService}{Einfacher Service}{Szenario 2: Erstellung eines einfachen Hello-World-Services.} 

\textbf{Szenario 3 - erweiterter Service:} Das dritte Szenario baut auf dem Zweiten auf und erweitert es um ein Datenmodell. Hier soll neben der Anbindung an eine Datenbank auch die Erstellung einer komplexeren API durchgeführt werden. Das Modell muss dabei als solches nicht komplex sein. Hier wird eine einfach Personal- und Aufgabenverwaltung umgesetzt. Dieses teilt Mitarbeiter in Abteilungen auf, welche wiederum Mitarbeiter als Abteilungsleiter haben. Zusätzlich können Mitarbeitern Aufgaben zugeordnet werden. Das vollständige Datenmodell ist im Bild \ref{Datenmodell} dargestellt.

\image[Szenario3Datenmodell.pdf][width=0.7\linewidth]{Datenmodell}{Datenmodell}{Szenario 3: Datenmodell der Aufgabenverwaltung}

Auf diesem Modell soll auch eine einfache Geschäftslogik implementiert werden, wie die Auflistung aller offenen Aufgaben nach Abteilungen. Die tatsächliche Geschäftslogik spielt dabei weniger eine Rolle. Vielmehr soll der Umgang mit Datenmodell und eigener Logik erprobt werden.

\image[Szenario3.pdf][width=0.7\linewidth]{ErweiterterService}{Service mit Datenmodell}{Szenario 3: Erstellung eines erweiterten Services.}
	
\newparagraph{Objektive Evaluation}
\
Die objektive Evaluation dient zur Findung der \textbf{harten} Daten. Hierzu werden die Artefakte aus der subjektiven Evaluation als Grundlage der Messungen genutzt. So können einzelne Funktionen im Code auf Komplexität untersucht werden, damit z.~B. eine Aussage über Umsetzungsaufwand oder Wartbarkeit getroffen werden kann. Neben diesen Messungen direkt an den Artefakten kann auch eine Performance Messung und Recherche zur Ermittlung der harten Daten herangezogen werden. 

\newsubparagraph{Performance Messung}

Der aus der subjektiven Evaluation gewonnene lauffähige Prototyp kann für Performance-Messungen herangezogen werden. Hierzu sollen ein oder mehrere Anforderungsprofile definiert werden, die eine reguläre Nutzung des Services unter verschiedenen Situationen betrachtet. Damit werden die Voraussetzungen und Umstände für die Messungen genauer definiert. Die Tabelle \ref{Anforderungsprofile} zeigt hierzu ein paar mögliche Beispiele. 

\begin{table}[!h]
	\centering
	\begin{tabular}{p{1cm}p{4cm}p{2cm}p{2cm}p{4cm}}
		\textbf{Nr.} & \textbf{Typ} & \textbf{Parallele Verbindungen} & 
		\textbf{Dauer} & \textbf{Besonderheit} \\
		\hline
		1 	& Datenservice 			& 255	&	30s		& CRUD Operationen auf Datenbank  \\
		\hline
		2	& Einfacher Service		& 255 	&	30s		& Wenig rechenintensive Geschäftslogik   \\
		\hline
		3	& Einfacher Service		& 10 	&	30s		& Wenig rechenintensive Geschäftslogik   \\
		\hline
		4	& Komplexer Service 	& 255	&	30s		& Stark rechenintensive Geschäftslogik  \\
		\hline
		5	& Komplexer Service 	& 10	&	30s		& Stark rechenintensive Geschäftslogik  \\
		\hline
	\end{tabular}
	\caption[Anforderungsprofile]{Beispiel: Anforderungsprofile für die objektiven Evaluation am Prototyp}
	\label{Anforderungsprofile}
\end{table}
\FloatBarrier

Das erste Anforderungsprofil-Beispiel stellt einen einfachen Datenservice dar. Dieser ist mit einer Datenbank verbunden und führt diese über eine \ac{REST}-Schnittstelle nach außen. Durch die viele parallele Anfragen wird dieser Service stark unter Last gestellt, um eine Aussage über Qualitätsmerkmale wie Stabilität, Fehlerrate und Durchsatz zu treffen.

Um möglichst limitierende Faktoren, wie die Datenbank, auszuschließen, wurden die Profile zwei bis fünf definiert. Diese bauen auf einer Geschäftslogik im Service auf. Dabei spielt es keine Rolle, ob ein sinnvolles Ergebnis berechnet wird. Mit der wenig rechenintensiven Logik steht der Overhead des Frameworks im Vordergrund. Ist dieser besonders groß, um z.~B. die übertragenen Daten zu de- und serialisieren, beeinflusst es jede Anfrage am Service und vermindert den Durchsatz. Die stark rechenintensive Logik zielt dabei mehr auf einen Vergleich zu anderen Programmiersprachen ab. Aus diesem Grund sollte die enthaltene Logik einheitlich sein. So kann sie darin bestehen, Testdaten zu generieren und diese zu sortieren.

Neben dem reinen Zeitverhalten können an den Profilen noch weitere Messwerte erhoben werden. Dies kann z.~B. Speicherbedarf in Ruhe sowie bei Vollauslastung sein. Dabei stellen die zuvor genannten Punkte nur Beispiele für Messungen an den Profilen dar. Welche genau durchgeführt werden, wurde bereits mit den Anforderungen über \ac{GQM} definiert.

\newsubparagraph{Recherche}

Auch durch eine Recherche können harte Daten gewonnen werden. So lassen sich Fragen beantworten, die in erster Linie nicht direkt mit der Entwicklung eines Microservices zusammenhängen. Bei einem Open-Source-Framework stellt sich z.~B. die Frage, ob dieses eine aktive Entwicklergemeinschaft hat und regelmäßig Fehler behoben werden. Sollte z.~B. die Behebung eines Fehlers viel Zeit in Anspruch nehmen, kann dies im produktiven Umfeld schnell zu Problemen führen. Auch ein kommerzieller Support kann gewünscht sein, wenn eigenes Fachwissen fehlt oder tiefgreifende Fehler schnell behoben werden sollen. Solche Punkte sind häufig mit ausschlaggebend bei der Entscheidung für ein Framework und sollten mit berücksichtigt werden.

\subsubsection{Metriken zuordnen}\label{Metriken_zuordnen}

Die in der Analysephase gefunden Metriken müssen den genauen Messpunkten zugeordnet werden. Wie in Abschnitt \ref{Evaluation_definieren} definiert, wird die Evaluation in objektiv und subjektiv unterschieden. Aus diesem Grund müssen die Metriken dahingehend differenziert werden. Die Objektiven lassen sich dabei am einfachsten erkennen, da sie harte Daten erheben. Diese können am Prototyp oder durch eine Recherche gewonnen werden.
Die Subjektiven basieren auf der Einschätzung des Prüfers und sollen in den Szenarien der subjektiven Evaluation ermittelt werden. Bild \ref{ZuordnungMetriken} zeigt diese Aufteilung. 

\image[MetrikenZuordnung.pdf][width=0.7\linewidth]{ZuordnungMetriken}{Zuordnung Metriken}{Aufteilung der Metriken in subjektiv und objektiv sowie Zuordnung zu einzelnen Messpunkten}

Wie bereits in Abschnitt \ref{Definition_Szenarien} erwähnt, kann es vorkommen, dass sich einzelne subjektive Metriken nicht zuordnen lassen. In diesem Fall müssen die Szenarien angepasst und verfeinert werden. 

\subsubsection{Evaluation durchführen}

An dieser Stelle werden die zuvor definierten Evaluationsprozesse durchgeführt. Dabei sollte der Fokus auf die Anforderungen mit hoher Priorität liegen. So kann die Evaluation frühzeitig abgebrochen werden, wenn wichtige Anforderungen nicht erfüllt werden.

Zuallererst werden die Szenarien des \acp{CW} umgesetzt. Dabei müssen die gewonnenen Erkenntnisse negativ als auch positiv protokolliert werden, damit diese sich später besser auswerten lassen. Sobald ein Szenario durchgeführt wurde, wird es durch die zugeordneten Metriken bewertet.

Nachdem das letzte Szenario de \ac{CW} durchgeführt wurde, steht ein Prototyp für die objektive Evaluation bereit. An diesem werden nun über den Programmcode die Softwaremetriken gemessen. Zusätzlich wird am laufenden Prototyp mittels Tools, wie z.~B. JMeter\footnote{http://jmeter.apache.org}, die Leistung sowie der Durchsatz gemessen.

\subsection{Abschlussphase}

\image[MFEM-Abschlussphase.pdf][width=0.4\linewidth]{MFEM-Abschlussphase}{MFEM-Abschlussphase}{Abschlussphase von \ac{MFEM}}

In der Abschlussphase geht es um die Aufbereitung und Präsentation der Daten. Die zuvor durchgeführten Phasen bringen viele Anforderungen und zugehörige Metriken hervor. Da dies einen schlechten Überblick über das Gesamtergebnis liefert, werden im nächsten Schritt die Daten aufbereitet.

\subsubsection{Ergebnisse auswerten}\label{Ergebnisse_auswerten}

Da Entscheidungsträger häufig keine Zeit für Detailanalysen haben, müssen die Daten so aufbereitet werden, dass mit einem Blick ein Gesamtergebnis sichtbar wird. Detailfragen lassen sich weiterhin durch die Anforderungen und Ergebnisse der Metriken klären. Ein sehr guter Überblick lässt sich durch ein Netzdiagramm erreichen. Hierzu können die Qualitätskategorien aus dem im Abschnitt \ref{Analyse_Wahl_Anforderungen} erstellten Quality Utility Tree für die Achsen genommen werden. Um einen Eindruck für das Ergebnis der Auswertung zu erhalten, ist ein Beispiel in Bild \ref{Netzdiagramm} dargestellt.

\image[Netzdiagramm.pdf][width=0.8\linewidth]{Netzdiagramm}{Netzdiagramm}{Beispiel für aufbereitet Daten als Netzdiagramm mit Qualitätskategorien als Achsen.}  

Die Achsen der einzelnen Qualitätskategorien stellen dabei den Gesamt-Erfüllungsgrad in Prozent dar. Wurden alle Anforderungen der Kategorie mit vollster Zufriedenheit erfüllt, ergibt sich ein Vollausschlag auf der jeweiligen Achse. Die Nichterfüllung einzelner Anforderungen mindert den Ausschlag entsprechend.

Wie stark einzelne Anforderungen in die zugehörige Kategorie einfließen, hängt von der Priorisierung dieser ab. Wurde eine Anforderung mit A bewertet, zählt das Ergebnis zu 100 Prozent. Entsprechend wird der Einfluss bei Priorität B und C auf 50 bzw. 25 Prozent gesenkt. Dies stellt sicher, dass die Nichterfüllung kleiner Anforderungen das Gesamtergebnis nicht zu stark nach unten ziehen. 

Für eine einfache Auswertung reicht die Unterscheidung zwischen Erfüllen (1) und Nichterfüllen (0) der Anforderungen. Somit reicht es die Gewichte der erfüllten Anforderungen pro Kategorie zu kumulieren, den Prozentsatz zu bilden und in die jeweilige Achse einzutragen. Dies hat jedoch den Nachteil, dass gerade bei den qualitativ erhobenen Daten viel Informationsgehalt verloren geht. Wird z.~B. die Lernbarkeit über einer Ordinalskala mit 3 möglichen Werten erhoben (sehr gut, gut, schlecht), geht die Differenzierung zwischen \enquote{sehr gut} und \enquote{gut} bzw. \enquote{gut} und \enquote{schlecht} verloren, je nachdem wo die Akzeptanzschwelle liegt.

Um dies zu verhindern, kann auch eine komplexere Auswertung erfolgen. Diese betrachten jede erhobene Metrik und stellt einen Erfüllungsgrad pro Anforderung zusammen. Hierzu wird das Ergebnis der Metrik auf einen Wert zwischen 0 und 1 normalisiert. Dies kann bei der zuvor genannten Ordinalskala mit 3 Werten folgende Ergebnisse enthalten:
  
\begin{table}[!h]
	\centering
	\begin{tabular}{ll}
		\textbf{Ergebnisse} & \textbf{Normalisierung}\\
		\hline
		sehr gut 	& 1  	\\
		\hline
		gut			& 0.5	\\
		\hline
		schlecht	& 0  	\\
		\hline
	\end{tabular}
	\caption[Ordinalskala Normalisierung]{Beispiel: Normalisierung einer Ordinalskala mit 3 Werten.}
	\label{Ordinalskala_Normalisierung}
\end{table}

Für quantitativ erhobene Daten kann weiterhin die dyadische Darstellung gewählt werden. Alternativ kann hierzu auch eine Abstufung mittels prozentualer Abweichung zum Zielwert ermittelt werden. Soll z.~B. die Antwortzeit eines ausgelasteten Services unter 10ms im Durchschnitt betragen, könnte folgende Normalisierung gewählt werden:

\begin{table}[!h]
	\centering
	\begin{tabular}{ll}
		\textbf{Ergebnisse} & \textbf{Normalisierung}\\
		\hline
		$\le 10ms$ 		& 1  	\\
		\hline
		$12ms$			& 0.8	\\
		\hline
		$14ms$			& 0.6  	\\
		\hline
		$16ms$			& 0.4  	\\
		\hline
		$18ms$			& 0.2  	\\
		\hline
		$\ge 20ms$		& 0  	\\
		\hline
	\end{tabular}
	\caption[Normalisierung quantitativer Daten]{Beispiel: Normalisierung von quantitativ erhobener Latenzzeit}
	\label{Normalisierung_quant}
\end{table}

Wie die erhobenen Daten normalisiert werden, hängt von den Anforderungen ab und welches Ziel diese verfolgen. Aus diesem Grund ist dies eine Einzelfallentscheidung und obliegt dem Prüfer.

Für den Erfüllungsgrad einer Anforderung können nun die einzelnen normalisierten Ergebnisse kumuliert und der Prozentsatz gebildet werden. Anschließend erfolgt die Multiplikation mit den Gewichten und das Eintragen in die jeweilige Achse analog zur einfachen Darstellung. 

Ein einfaches Beispiel für die Auswertung einer Kategorie findet sich in Bild \ref{AuswertungBeispiel}

\image[AuswertungBeispiel.pdf]{AuswertungBeispiel}{Auswertung Beispiel}{Beispiel für die Auswertung einer Kategorie.}

Die so ausgewerteten und in ein Netzdiagramm eingetragenen Daten bieten einen guten Überblick über die Qualität eines Frameworks. Sie lassen aber auch einen Vergleich mehrerer Frameworks zu, indem verschiedene Ergebnisse in das Diagramm eingetragen werden.

\subsubsection{Resultat präsentieren}

Im letzten Schritt werden die Ergebnisse festgehalten und möglichen Stakeholdern präsentiert. Dies muss dabei nicht zwangsläufig eine Präsentation sein. Viel mehr geht es darum, aus dem Gesamtergebnis ein Resultat zu ziehen. Dies kann in einem abschließendem Dokument erfolgen. Dabei lassen sich wichtige Anforderungen, die nicht erfüllt wurden hervorheben. Sollten z.~B. Funktionen fehlen, kann bei einem späteren Release des Frameworks die Bewertung verkürzt wiederholt und nur auf diese spezifischen Anforderungen untersucht werden.
Auch kann anhand des zuvor gewonnen Gesamtüberblicks ein Vergleich mit bereits bewerteten Frameworks erfolgen. Dies kann anschließend die Grundlage für oder gegen die weitere Entwicklung mit dem Framework sein.

\pagebreak












